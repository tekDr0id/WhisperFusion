You're hitting the same spot every time: the image builds fine, the container starts, your script gets past CUDA, then Python touches MPI and Open MPI dies with

> `opal_shmem_base_select failed`
> `--> Returned value -1 instead of OPAL_SUCCESS`
> container exits 1. 

That‚Äôs not a ‚Äúmodel‚Äù problem. It‚Äôs the runtime MPI layer inside the container.

Let me unpack what I see and what I‚Äôd do.

## 1. What‚Äôs actually failing

* Your runtime script `build-models.sh` calls `build-whisper.sh` because the model dir is empty. ‚úî 
* `build-whisper.sh` does a smart CUDA fallback. That part ends up working after it installs `cuda-python==12.4.0`. ‚úî 
* After CUDA, the script starts its ‚ÄúSMART MPI FALLBACK STRATEGY.‚Äù That strategy tries a bunch of Open MPI knobs (tcp/self, exclude vader, mpirun wrapper, comprehensive BTL exclusion, single node). All of them fail. The reason is not your knob choice. Open MPI itself can‚Äôt init in this container. 
* At the end, it runs `ompi_info --all | grep btl` and then `python3 build.py ...` which again pulls in MPI and dies. That‚Äôs the final ‚Äúüí• ALL PROVEN MPI APPROACHES FAILED‚Äù you see. 

So: the image is ok (build log ends with ‚ÄúWhisperFusion setup completed successfully!‚Äù). The runtime is not. 

## 2. Why Open MPI fails in containers like this

Inside your image you installed a full Ubuntu MPI stack:

* `openmpi-bin`, `openmpi-common`, `libopenmpi-dev`, `libpmix-dev`, `libucx0`, etc. during `apt` in the Dockerfile. That‚Äôs a full distro Open MPI 4.1.2. 

But at runtime, Open MPI wants a working shared-memory (shmem) backend. In Docker on Windows/Linux, that often breaks because:

1. `/dev/shm` is too small or not mounted as tmpfs for the container.
2. IPC namespaces aren‚Äôt what Open MPI expects.
3. You run as root inside container with Open MPI‚Äôs safety checks.
4. You have a distro Open MPI inside the image that doesn‚Äôt quite match the container runtime environment.

When that happens, Open MPI never gets to the ‚Äúyour flags are wrong‚Äù stage. It just bails in `opal_init`, which is exactly what you have. 

Your script tries to fix it by twiddling MCA vars (`OMPI_MCA_btl`, `OMPI_MCA_pml`, excluding `vader`, etc.). That‚Äôs good practice, but it only helps if the transport is fine and you just picked a bad BTL. Here the base init is bad, so all five approaches fail. So the script is doing too much for an environment that‚Äôs fundamentally not MPI-friendly. 

## 3. Fastest path to green: stop using MPI for this build

Look at this function in your script:

```sh
# Function to test TensorRT build with different MPI configurations
test_tensorrt_build() {
    ...
    if timeout 60s python3 build.py ...; then
        ...
    else
        ...
    fi
}
```

Even the ‚Äúno-mpirun‚Äù path still executes `python3 build.py ...`. Your Python environment has `mpi4py` and an MPI-capable TensorRT-LLM stack installed during the image build (I can see a big `mpi4py` download in the build log). So at runtime, `python3 build.py` very likely imports `mpi4py` or code that initializes MPI, and that hits the same broken Open MPI. 

**So one very pragmatic fix**: right before you run `python3 build.py` in `build-whisper.sh`, uninstall or neutralize MPI in that container.

Something like:

```sh
# add near the top of build-whisper.sh, before MPI diagnostics
uv pip uninstall --system -y mpi4py 2>/dev/null || true
apt-get remove -y openmpi-bin libopenmpi-dev libopenmpi3 2>/dev/null || true
```

Then run `python3 build.py ...` without all the MPI gymnastics.

That turns the build into ‚Äúsingle-process, in-container‚Äù which is enough to generate your Whisper TensorRT engine.

This is the simplest, least docker-compose-y change and it matches what you actually need right now: ‚Äúmake the container not exit 1 on startup.‚Äù

## 4. If you actually want MPI (multi-GPU, real TensorRT-LLM style)

Then we fix the container runtime instead of the script.

Add this to your `docker-compose.yml` service for `whisperfusion`:

```yaml
services:
  whisperfusion:
    shm_size: 1gb      # give Open MPI real /dev/shm
    ipc: host          # let it see host IPC
    privileged: true   # often needed for HPC-y stacks in Docker Desktop
    environment:
      - OMPI_ALLOW_RUN_AS_ROOT=1
      - OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
      - OMPI_MCA_btl=tcp,self
      - OMPI_MCA_pml=ob1
      - OMPI_MCA_shmem=^mmap
```

Why these?

* `shm_size` and `ipc: host` tend to make `opal_shmem_base_select` happy in containers.
* You‚Äôre already exporting `OMPI_ALLOW_RUN_AS_ROOT` in the script. Put it in compose so it‚Äôs always there. 
* `OMPI_MCA_shmem=^mmap` is a useful extra hammer for ‚Äúshmem selection failed‚Äù cases.

If you do this, your current script‚Äôs five attempts are more likely to succeed without you having to rip out MPI.

## 5. Dockerfile cleanup I‚Äôd do

Your build log shows a pretty heavy image:

* base: `nvcr.io/nvidia/pytorch:24.03-py3`
* 200+ apt packages including `openmpi`, `ffmpeg`, desktop-ish libs. 
* then a gigantic `uv pip` step pulling TensorRT 10, TensorRT-LLM 0.10.0, CUDA 12.x wheels. 

So right now you‚Äôre paying for MPI inside the image even though it crashes at runtime.

I‚Äôd split this:

1. **build stage**: install openmpi and mpi4py to let you build whatever needs MPI.
2. **runtime stage**: copy only the built artifacts (your `whisper_small_en` engine, model, scripts) into a slimmer image that doesn‚Äôt have openmpi at all.

That way your container start doesn‚Äôt even see MPI, so it can‚Äôt crash.

## 6. Your script is doing more work than the environment can support

The CUDA part is smart and it actually succeeded. That tells us your base GPU tooling inside the image is fine. The only real outlier is MPI. So I‚Äôd refactor `build-whisper.sh` roughly like this:

1. run CUDA diagnostics (keep)
2. try `python3 build.py ...` **once**
3. if error message mentions MPI / Open MPI / opal, then:

   * uninstall mpi4py and re-run
   * if still bad, print a ‚Äúyou need ipc: host + bigger /dev/shm‚Äù message and exit nonzero

Right now the script tries 5 MPI approaches that can‚Äôt work in this container, which just makes the logs noisy. You saw it loop through them. 

## 7. Things you might be assuming (that I‚Äôd question)

* **Assumption:** ‚ÄúTensorRT-LLM build must use MPI.‚Äù
  Not always. For a single small Whisper engine, you can often just run the builder in one process.

* **Assumption:** ‚ÄúFix is inside Python.‚Äù
  Not in this case. The error is from the native Open MPI layer before Python can do much.

* **Assumption:** ‚ÄúThe image is the problem because the build log is long.‚Äù
  Build succeeded. Runtime environment is the problem. Different place. 

## 8. A better prompt you could‚Äôve given me

Something like:

> ‚ÄúI have a Dockerized Whisper/TensorRT build. Image builds fine. At container start a shell script runs `python3 build.py ...`. That script imports mpi4py/Open MPI and dies with `opal_shmem_base_select failed` inside the container. I‚Äôm on Docker Desktop (Windows). Show me the simplest way to: 1) make the container not crash, 2) optionally keep MPI working.‚Äù

That would have told me up front that we can just remove MPI from runtime or grow `/dev/shm`.

---

## TL;DR recommendations

1. **Quick win (do this first):** uninstall MPI in the runtime script before build:

   ```sh
   uv pip uninstall --system -y mpi4py || true
   apt-get remove -y openmpi-bin libopenmpi3 libopenmpi-dev || true
   python3 build.py ...
   ```

   That should stop the container from exiting 1.

2. **If you want MPI to work instead of removing it:** update `docker-compose.yml` for that service with `shm_size: 1gb`, `ipc: host`, and the OMPI env vars above.

3. **Longer term:** build MPI in a build stage, keep runtime MPI-free.

If you tell me which of those 3 paths you want (no-MPI, fix-MPI, or slim-image), I can write the exact `Dockerfile`/compose diff next.
